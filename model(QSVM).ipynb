{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZHDbqbXH-Ny"
   },
   "source": [
    "## multiclass variational classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q7r76FEZIBuH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_size = 512\n",
    "data_path = 'train_fBm_dataset_shuffeld_L'+str(L_size)+'.csv'\n",
    "\n",
    "num_layers = 18\n",
    "total_iterations =600\n",
    "num_classes = 10  \n",
    "margin = 0.0375\n",
    "batch_size = 10\n",
    "lr_adam = 0.01\n",
    "train_split = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuF1H0bNvAzm"
   },
   "source": [
    "# defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FCSXAvDxyCAK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def layer(W):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "    for j in range(num_qubits - 1):\n",
    "        qml.CNOT(wires=[j, j + 1])\n",
    "    if num_qubits >= 2:\n",
    "       \n",
    "        qml.CNOT(wires=[num_qubits - 1, 0])\n",
    "        \n",
    "###################################################################\n",
    "def circuit(weights, feat=None):  #feat==feature\n",
    "    qml.AmplitudeEmbedding(feat, range(num_qubits), pad_with=0.0, normalize=True)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "########################################################################\n",
    "def variational_classifier(q_circuit, params, feat):\n",
    "    weights = params[0]\n",
    "    bias = params[1]\n",
    "    return q_circuit(weights, feat=feat) + bias\n",
    "\n",
    "########################################################################\n",
    "def multiclass_svm_loss(q_circuits, all_params, feature_vecs, true_labels):\n",
    "    loss = 0\n",
    "    num_samples = len(true_labels)\n",
    "    for i, feature_vec in enumerate(feature_vecs):\n",
    "     \n",
    "        s_true = variational_classifier(q_circuits[int(true_labels[i])],\n",
    "            (all_params[0][int(true_labels[i])], all_params[1][int(true_labels[i])]),feature_vec,)\n",
    "        s_true = s_true.float()\n",
    "        li = 0\n",
    "        for j in range(num_classes):\n",
    "            if j != int(true_labels[i]):\n",
    "                s_j = variational_classifier(q_circuits[j], (all_params[0][j], all_params[1][j]), feature_vec)\n",
    "                s_j = s_j.float()\n",
    "                \n",
    "                li += torch.max(torch.zeros(1).float(), s_j - s_true + margin)\n",
    "        loss += li\n",
    "\n",
    "    return loss / num_samples\n",
    "########################################################################\n",
    "def classify(q_circuits, all_params, feature_vecs, labels):\n",
    "    predicted_labels = []\n",
    "    for i, feature_vec in enumerate(feature_vecs):\n",
    "        scores = np.zeros(num_classes)\n",
    "        for c in range(num_classes):\n",
    "            score = variational_classifier(\n",
    "                q_circuits[c], (all_params[0][c], all_params[1][c]), feature_vec\n",
    "            )\n",
    "            scores[c] = float(score)\n",
    "        pred_class = np.argmax(scores)\n",
    "        predicted_labels.append(pred_class)\n",
    "    return predicted_labels\n",
    "########################################################################\n",
    "\n",
    "def accuracy(labels, hard_predictions):\n",
    "    correct = 0\n",
    "    for l, p in zip(labels, hard_predictions):\n",
    "        if torch.abs(l - p) < 1e-5:\n",
    "            correct = correct + 1\n",
    "    accuracy_amount= correct / labels.shape[0]\n",
    "    return accuracy_amount\n",
    "########################################################################\n",
    "#load and normalize each input\n",
    "def load_and_process_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    X = data.iloc[:,1:-1].values \n",
    "    Y = data.iloc[:,-1].values\n",
    "    X=(X-np.mean(X,axis=1).reshape(-1,1))/(np.std(X,axis=1).reshape(-1,1))\n",
    "    Y = torch.tensor(Y)\n",
    "    X = torch.tensor(X)\n",
    "    Y.shape,X.shape\n",
    "    return X, Y\n",
    "########################################################################\n",
    "# Create a train and test split.\n",
    "def split_data(feature_vecs, Y):\n",
    "    num_data = len(Y)\n",
    "    num_train = int(train_split * num_data)\n",
    "    index = np.random.permutation(range(num_data))\n",
    "    feat_vecs_train = feature_vecs[index[:num_train]]\n",
    "    Y_train = Y[index[:num_train]]\n",
    "    feat_vecs_test = feature_vecs[index[num_train:]]\n",
    "    Y_test = Y[index[num_train:]]\n",
    "    return feat_vecs_train, feat_vecs_test, Y_train, Y_test\n",
    "########################################################################\n",
    "def training(features, Y, rerun):\n",
    "    num_data = Y.shape[0]\n",
    "    feat_vecs_train, feat_vecs_test, Y_train, Y_test = split_data(features, Y)\n",
    "    num_train = Y_train.shape[0]\n",
    "    q_circuits = qnodes\n",
    "\n",
    "    results_file=pd.read_excel('results.xlsx')\n",
    "\n",
    "    if rerun==True:\n",
    "       \n",
    "        with open('best_params' + str(L_size) + '.pickle', 'rb') as f:\n",
    "            loaded_params = pickle.load(f)\n",
    "        loaded_weights, loaded_bias = loaded_params\n",
    "        all_weights = [\n",
    "        Variable(torch.tensor(w, dtype=torch.float32), requires_grad=True) \n",
    "        for w in loaded_weights\n",
    "        ]\n",
    "        all_bias = [\n",
    "        Variable(torch.tensor(b, dtype=torch.float32), requires_grad=True)\n",
    "        for b in loaded_bias\n",
    "        ]\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        optimizer = optim.Adam(all_weights + all_bias, lr=lr_adam)\n",
    "\n",
    "        # Final params tuple\n",
    "        params = (all_weights, all_bias)\n",
    "        print(\"Num params: \", 3 * num_layers * num_qubits * 3 + 3)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        # Initialize the parameters\n",
    "        all_weights = [\n",
    "            Variable(0.1 * torch.randn(num_layers, num_qubits, 3), requires_grad=True)\n",
    "            for i in range(num_classes)\n",
    "        ]\n",
    "        all_bias = [Variable(0.1 * torch.ones(1), requires_grad=True) for i in range(num_classes)]\n",
    "        optimizer = optim.Adam(all_weights + all_bias, lr=lr_adam)\n",
    "        params = (all_weights, all_bias)\n",
    "        print(\"Num params: \", 3 * num_layers * num_qubits * 3 + 3)\n",
    "\n",
    "    costs, train_acc, test_acc = [], [], []\n",
    "    print(\"#############################################################################\")\n",
    "    for it in range(total_iterations):\n",
    "        start=time.time()\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "        feat_vecs_train_batch = feat_vecs_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        curr_cost = multiclass_svm_loss(q_circuits, params, feat_vecs_train_batch, Y_train_batch)\n",
    "        curr_cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute predictions on train and validation set\n",
    "        predictions_train = classify(q_circuits, params, feat_vecs_train, Y_train)\n",
    "        predictions_test =classify(q_circuits, params, feat_vecs_test, Y_test)\n",
    "        acc_train = accuracy(Y_train, predictions_train)\n",
    "        acc_test = accuracy(Y_test, predictions_test)\n",
    "                                                            \n",
    "        new_row=pd.DataFrame({'Iter': [it + 1], 'Cost':[curr_cost.item()],'Acc train':[acc_train],'Acc test':[acc_test]})\n",
    "        results_file=pd.concat([results_file,new_row],ignore_index=True)\n",
    "        results_file.to_excel('results.xlsx',index=False)\n",
    "        end=time.time()\n",
    "        print(\n",
    "            \"Iter: {:5d} | time:{:0.5f} |Cost: {:0.7f} | Acc train: {:0.7f} | Acc test: {:0.7f} \"\n",
    "            \"\".format(it + 1,end-start ,curr_cost.item(), acc_train, acc_test)\n",
    "        )\n",
    "        with open('best_params' + str(L_size) + '.pickle', 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knXVzNOKhwBs"
   },
   "source": [
    "# Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of ech time serie: 512\n",
      "num_qubits : 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "feature_size= len(data.iloc[[0]].values[0][1:-1])\n",
    "print(\"length of ech time serie:\",feature_size)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "############################################################\n",
    "\n",
    "# the number of the required qubits is calculated from the number of features\n",
    "num_qubits = int(np.ceil(np.log2(feature_size)))\n",
    "\n",
    "print(\"num_qubits :\",num_qubits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wx0Dat_T8a79",
    "outputId": "86779aff-71e0-4afd-d061-aaec077735ed",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([990]), torch.Size([990, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, Y = load_and_process_data(data_path)\n",
    "Y.shape,features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training model  and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<default.qubit device (wires=9) at 0x7e45f0089fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>,\n",
       " <QNode: device='<default.qubit device (wires=9) at 0x7e45f0089fd0>', interface='torch', diff_method='best'>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnodes = []\n",
    "for iq in range(num_classes):\n",
    "    qnode = qml.QNode(circuit, dev, interface=\"torch\")  #The interface that will be used for classical backpropagation\n",
    "    qnodes.append(qnode)                                #This affects the types of objects that can be passed to/returned from the QNode\n",
    "q_circuits = qnodes\n",
    "q_circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  1461\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parameters\n",
    "print(\"Num params: \", 3 * num_layers * num_qubits * 3 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMySJdfFxXFT",
    "outputId": "96503d3d-0a3c-4eeb-84c5-c7360cfadaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  1461\n",
      "#############################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84956/679654816.py:149: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_file=pd.concat([results_file,new_row],ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | time:2654.94567 |Cost: 0.4286701 | Acc train: 0.1159030 | Acc test: 0.1209677 \n",
      "Iter:     2 | time:2675.14546 |Cost: 0.4187289 | Acc train: 0.1172507 | Acc test: 0.1451613 \n",
      "Iter:     3 | time:2652.61991 |Cost: 0.4008683 | Acc train: 0.1307278 | Acc test: 0.1491935 \n",
      "Iter:     4 | time:2635.50237 |Cost: 0.4497766 | Acc train: 0.1778976 | Acc test: 0.1572581 \n",
      "Iter:     5 | time:2652.13475 |Cost: 0.6820160 | Acc train: 0.1940701 | Acc test: 0.1653226 \n",
      "Iter:     6 | time:2652.67882 |Cost: 0.4064651 | Acc train: 0.1954178 | Acc test: 0.1733871 \n",
      "Iter:     7 | time:2652.89472 |Cost: 0.4200077 | Acc train: 0.2075472 | Acc test: 0.1814516 \n",
      "Iter:     8 | time:2654.40010 |Cost: 0.3346764 | Acc train: 0.1415094 | Acc test: 0.1129032 \n",
      "Iter:     9 | time:2656.93116 |Cost: 0.2865305 | Acc train: 0.1388140 | Acc test: 0.1048387 \n",
      "Iter:    10 | time:2638.89285 |Cost: 0.3352032 | Acc train: 0.1617251 | Acc test: 0.1451613 \n",
      "Iter:    11 | time:2650.97592 |Cost: 0.4365485 | Acc train: 0.1752022 | Acc test: 0.1572581 \n",
      "Iter:    12 | time:2650.23382 |Cost: 0.3126461 | Acc train: 0.1846361 | Acc test: 0.1653226 \n",
      "Iter:    13 | time:2651.74483 |Cost: 0.5277705 | Acc train: 0.1832884 | Acc test: 0.1733871 \n",
      "Iter:    14 | time:2660.86626 |Cost: 0.6559235 | Acc train: 0.1832884 | Acc test: 0.1572581 \n",
      "Iter:    15 | time:2654.96884 |Cost: 0.6392091 | Acc train: 0.1819407 | Acc test: 0.1572581 \n",
      "Iter:    16 | time:2649.87521 |Cost: 0.5312306 | Acc train: 0.1832884 | Acc test: 0.1411290 \n",
      "Iter:    17 | time:2654.34831 |Cost: 0.2704909 | Acc train: 0.2075472 | Acc test: 0.1451613 \n",
      "Iter:    18 | time:2652.96371 |Cost: 0.4618658 | Acc train: 0.2021563 | Acc test: 0.1370968 \n",
      "Iter:    19 | time:2640.29284 |Cost: 0.3377446 | Acc train: 0.2021563 | Acc test: 0.1290323 \n",
      "Iter:    20 | time:2654.91214 |Cost: 0.2227711 | Acc train: 0.2210243 | Acc test: 0.1411290 \n",
      "Iter:    21 | time:2657.40036 |Cost: 0.3295636 | Acc train: 0.1994609 | Acc test: 0.1491935 \n",
      "Iter:    22 | time:2643.82191 |Cost: 0.2276373 | Acc train: 0.1832884 | Acc test: 0.1532258 \n",
      "Iter:    23 | time:2646.06092 |Cost: 0.3171105 | Acc train: 0.2008086 | Acc test: 0.1532258 \n",
      "Iter:    24 | time:2643.51340 |Cost: 0.1520528 | Acc train: 0.2061995 | Acc test: 0.1572581 \n",
      "Iter:    25 | time:2644.78096 |Cost: 0.3914821 | Acc train: 0.2115903 | Acc test: 0.1572581 \n",
      "Iter:    26 | time:2640.85817 |Cost: 0.3515793 | Acc train: 0.2129380 | Acc test: 0.1491935 \n",
      "Iter:    27 | time:2641.91145 |Cost: 0.1868049 | Acc train: 0.2196765 | Acc test: 0.1532258 \n",
      "Iter:    28 | time:2641.63474 |Cost: 0.1766313 | Acc train: 0.2183288 | Acc test: 0.1612903 \n",
      "Iter:    29 | time:2646.22716 |Cost: 0.2398677 | Acc train: 0.2412399 | Acc test: 0.1854839 \n",
      "Iter:    30 | time:2650.33438 |Cost: 0.2332428 | Acc train: 0.2547170 | Acc test: 0.1854839 \n",
      "Iter:    31 | time:2645.48355 |Cost: 0.1200453 | Acc train: 0.2708895 | Acc test: 0.1814516 \n",
      "Iter:    32 | time:2647.26220 |Cost: 0.3335765 | Acc train: 0.2816712 | Acc test: 0.1693548 \n",
      "Iter:    33 | time:2642.06838 |Cost: 0.2095787 | Acc train: 0.2884097 | Acc test: 0.1572581 \n",
      "Iter:    34 | time:2643.50653 |Cost: 0.2135741 | Acc train: 0.2938005 | Acc test: 0.1572581 \n",
      "Iter:    35 | time:2643.13127 |Cost: 0.3859453 | Acc train: 0.3005391 | Acc test: 0.1653226 \n",
      "Iter:    36 | time:2640.58932 |Cost: 0.2598609 | Acc train: 0.2938005 | Acc test: 0.1814516 \n",
      "Iter:    37 | time:2641.60681 |Cost: 0.1994469 | Acc train: 0.2964960 | Acc test: 0.1693548 \n",
      "Iter:    38 | time:2649.46858 |Cost: 0.1215993 | Acc train: 0.2574124 | Acc test: 0.1612903 \n",
      "Iter:    39 | time:2640.40667 |Cost: 0.2409568 | Acc train: 0.2560647 | Acc test: 0.1693548 \n",
      "Iter:    40 | time:2642.34283 |Cost: 0.2349761 | Acc train: 0.2479784 | Acc test: 0.1653226 \n",
      "Iter:    41 | time:2636.66110 |Cost: 0.3065863 | Acc train: 0.2479784 | Acc test: 0.1572581 \n",
      "Iter:    42 | time:2649.16746 |Cost: 0.1513620 | Acc train: 0.2601078 | Acc test: 0.1532258 \n",
      "Iter:    43 | time:2641.39598 |Cost: 0.1255283 | Acc train: 0.2803235 | Acc test: 0.1612903 \n",
      "Iter:    44 | time:2641.16263 |Cost: 0.3224249 | Acc train: 0.3032345 | Acc test: 0.1653226 \n",
      "Iter:    45 | time:2643.33367 |Cost: 0.2679809 | Acc train: 0.2964960 | Acc test: 0.2056452 \n",
      "Iter:    46 | time:2642.30248 |Cost: 0.1869910 | Acc train: 0.2735849 | Acc test: 0.1895161 \n",
      "Iter:    47 | time:2642.76868 |Cost: 0.2167963 | Acc train: 0.2681941 | Acc test: 0.1895161 \n",
      "Iter:    48 | time:2649.01495 |Cost: 0.2488420 | Acc train: 0.2722372 | Acc test: 0.1975806 \n",
      "Iter:    49 | time:2641.74282 |Cost: 0.1437984 | Acc train: 0.2722372 | Acc test: 0.2056452 \n",
      "Iter:    50 | time:2643.22769 |Cost: 0.1082879 | Acc train: 0.2749326 | Acc test: 0.1895161 \n",
      "Iter:    51 | time:2644.48566 |Cost: 0.3112698 | Acc train: 0.3018868 | Acc test: 0.1975806 \n",
      "Iter:    52 | time:2633.64299 |Cost: 0.4683105 | Acc train: 0.3113208 | Acc test: 0.2016129 \n",
      "Iter:    53 | time:2639.14474 |Cost: 0.5418301 | Acc train: 0.3247978 | Acc test: 0.1935484 \n",
      "Iter:    54 | time:2636.15134 |Cost: 0.3093084 | Acc train: 0.3315364 | Acc test: 0.1935484 \n",
      "Iter:    55 | time:2640.77786 |Cost: 0.2743613 | Acc train: 0.3355795 | Acc test: 0.2177419 \n",
      "Iter:    56 | time:2642.73685 |Cost: 0.2076219 | Acc train: 0.3153639 | Acc test: 0.1774194 \n",
      "Iter:    57 | time:2647.76942 |Cost: 0.1664648 | Acc train: 0.3032345 | Acc test: 0.1451613 \n",
      "Iter:    58 | time:2640.82424 |Cost: 0.3099755 | Acc train: 0.2803235 | Acc test: 0.1290323 \n",
      "Iter:    59 | time:2644.00793 |Cost: 0.2037246 | Acc train: 0.2668464 | Acc test: 0.1048387 \n",
      "Iter:    60 | time:2646.78428 |Cost: 0.1926947 | Acc train: 0.2614555 | Acc test: 0.0927419 \n",
      "Iter:    61 | time:2640.71723 |Cost: 0.2048787 | Acc train: 0.2614555 | Acc test: 0.1008065 \n",
      "Iter:    62 | time:2652.57210 |Cost: 0.2229170 | Acc train: 0.2641509 | Acc test: 0.1088710 \n",
      "Iter:    63 | time:2649.71185 |Cost: 0.1429671 | Acc train: 0.2681941 | Acc test: 0.1169355 \n",
      "Iter:    64 | time:2642.73663 |Cost: 0.2965079 | Acc train: 0.2681941 | Acc test: 0.1250000 \n",
      "Iter:    65 | time:2641.60104 |Cost: 0.2531195 | Acc train: 0.2735849 | Acc test: 0.1290323 \n",
      "Iter:    66 | time:2642.01534 |Cost: 0.2743385 | Acc train: 0.2789757 | Acc test: 0.1209677 \n",
      "Iter:    67 | time:2634.19707 |Cost: 0.2723715 | Acc train: 0.2870620 | Acc test: 0.1330645 \n",
      "Iter:    68 | time:2636.58069 |Cost: 0.2128663 | Acc train: 0.2911051 | Acc test: 0.1451613 \n",
      "Iter:    69 | time:2643.69173 |Cost: 0.6502258 | Acc train: 0.3059299 | Acc test: 0.1653226 \n",
      "Iter:    70 | time:2644.58831 |Cost: 0.1202896 | Acc train: 0.3059299 | Acc test: 0.1814516 \n",
      "Iter:    71 | time:2642.46418 |Cost: 0.2763616 | Acc train: 0.3180593 | Acc test: 0.1895161 \n",
      "Iter:    72 | time:2642.88326 |Cost: 0.1622006 | Acc train: 0.3207547 | Acc test: 0.2096774 \n",
      "Iter:    73 | time:2637.73979 |Cost: 0.1631929 | Acc train: 0.3167116 | Acc test: 0.2096774 \n",
      "Iter:    74 | time:2636.67754 |Cost: 0.3069920 | Acc train: 0.3140162 | Acc test: 0.1895161 \n",
      "Iter:    75 | time:2643.22426 |Cost: 0.2429170 | Acc train: 0.3167116 | Acc test: 0.1935484 \n",
      "Iter:    76 | time:2644.66777 |Cost: 0.1795642 | Acc train: 0.3113208 | Acc test: 0.1895161 \n",
      "Iter:    77 | time:2653.30416 |Cost: 0.1362673 | Acc train: 0.3194070 | Acc test: 0.1895161 \n",
      "Iter:    78 | time:2641.87250 |Cost: 0.2342140 | Acc train: 0.3247978 | Acc test: 0.1935484 \n",
      "Iter:    79 | time:2649.89240 |Cost: 0.2113898 | Acc train: 0.3598383 | Acc test: 0.1814516 \n",
      "Iter:    80 | time:2642.02512 |Cost: 0.1490413 | Acc train: 0.3706199 | Acc test: 0.2016129 \n",
      "Iter:    81 | time:2637.49119 |Cost: 0.2318269 | Acc train: 0.3652291 | Acc test: 0.2258065 \n",
      "Iter:    82 | time:2641.61795 |Cost: 0.1759621 | Acc train: 0.3517520 | Acc test: 0.2217742 \n",
      "Iter:    83 | time:2637.68032 |Cost: 0.1436520 | Acc train: 0.3530997 | Acc test: 0.2177419 \n",
      "Iter:    84 | time:2644.39026 |Cost: 0.1822796 | Acc train: 0.3530997 | Acc test: 0.2016129 \n",
      "Iter:    85 | time:2650.54197 |Cost: 0.1421814 | Acc train: 0.3450135 | Acc test: 0.2177419 \n",
      "Iter:    86 | time:2700.27544 |Cost: 0.2803946 | Acc train: 0.3355795 | Acc test: 0.2258065 \n",
      "Iter:    87 | time:2902.42305 |Cost: 0.1140606 | Acc train: 0.3423181 | Acc test: 0.2217742 \n",
      "Iter:    88 | time:3928.40132 |Cost: 0.1726283 | Acc train: 0.3450135 | Acc test: 0.1975806 \n",
      "Iter:    89 | time:3937.56109 |Cost: 0.3600014 | Acc train: 0.3153639 | Acc test: 0.1935484 \n",
      "Iter:    90 | time:3909.32929 |Cost: 0.2813329 | Acc train: 0.3315364 | Acc test: 0.1895161 \n",
      "Iter:    91 | time:3918.94600 |Cost: 0.0644841 | Acc train: 0.3490566 | Acc test: 0.1693548 \n",
      "Iter:    92 | time:3907.56575 |Cost: 0.1883769 | Acc train: 0.3611860 | Acc test: 0.1854839 \n",
      "Iter:    93 | time:3920.17170 |Cost: 0.1350199 | Acc train: 0.3504043 | Acc test: 0.1854839 \n",
      "Iter:    94 | time:3912.35149 |Cost: 0.2211165 | Acc train: 0.3369272 | Acc test: 0.2056452 \n",
      "Iter:    95 | time:3922.58597 |Cost: 0.2561270 | Acc train: 0.3490566 | Acc test: 0.2056452 \n",
      "Iter:    96 | time:3922.98141 |Cost: 0.3780265 | Acc train: 0.3396226 | Acc test: 0.1975806 \n",
      "Iter:    97 | time:3913.04427 |Cost: 0.0818515 | Acc train: 0.3436658 | Acc test: 0.2137097 \n",
      "Iter:    98 | time:3920.05605 |Cost: 0.1845684 | Acc train: 0.3436658 | Acc test: 0.2177419 \n",
      "Iter:    99 | time:3908.78658 |Cost: 0.2190644 | Acc train: 0.3517520 | Acc test: 0.2177419 \n",
      "Iter:   100 | time:3916.85730 |Cost: 0.0706564 | Acc train: 0.3490566 | Acc test: 0.2379032 \n",
      "Iter:   101 | time:3918.76446 |Cost: 0.1496838 | Acc train: 0.3598383 | Acc test: 0.2258065 \n",
      "Iter:   102 | time:3918.10202 |Cost: 0.2774298 | Acc train: 0.3625337 | Acc test: 0.2217742 \n",
      "Iter:   103 | time:3922.98609 |Cost: 0.1386622 | Acc train: 0.3638814 | Acc test: 0.2096774 \n",
      "Iter:   104 | time:3914.55272 |Cost: 0.1720715 | Acc train: 0.3733154 | Acc test: 0.2177419 \n",
      "Iter:   105 | time:3922.21095 |Cost: 0.2462339 | Acc train: 0.3760108 | Acc test: 0.2056452 \n",
      "Iter:   106 | time:3921.58951 |Cost: 0.2572830 | Acc train: 0.3773585 | Acc test: 0.2096774 \n",
      "Iter:   107 | time:3914.62677 |Cost: 0.2266813 | Acc train: 0.3894879 | Acc test: 0.2177419 \n",
      "Iter:   108 | time:3924.22598 |Cost: 0.1388386 | Acc train: 0.3908356 | Acc test: 0.2096774 \n",
      "Iter:   109 | time:3912.87599 |Cost: 0.2937501 | Acc train: 0.3719677 | Acc test: 0.2016129 \n",
      "Iter:   110 | time:3911.92862 |Cost: 0.2036839 | Acc train: 0.3638814 | Acc test: 0.1935484 \n",
      "Iter:   111 | time:3914.00185 |Cost: 0.2304766 | Acc train: 0.3665768 | Acc test: 0.1975806 \n",
      "Iter:   112 | time:3916.88604 |Cost: 0.1824524 | Acc train: 0.3638814 | Acc test: 0.2056452 \n",
      "Iter:   113 | time:3912.40431 |Cost: 0.1805461 | Acc train: 0.3638814 | Acc test: 0.2096774 \n",
      "Iter:   114 | time:3924.65406 |Cost: 0.2346270 | Acc train: 0.3611860 | Acc test: 0.2137097 \n",
      "Iter:   115 | time:3913.70936 |Cost: 0.1158966 | Acc train: 0.3719677 | Acc test: 0.2137097 \n",
      "Iter:   116 | time:3918.22890 |Cost: 0.1572956 | Acc train: 0.3719677 | Acc test: 0.2096774 \n",
      "Iter:   117 | time:3930.05262 |Cost: 0.1897134 | Acc train: 0.3584906 | Acc test: 0.2177419 \n",
      "Iter:   118 | time:3943.54305 |Cost: 0.2069717 | Acc train: 0.3490566 | Acc test: 0.2177419 \n",
      "Iter:   119 | time:3912.05079 |Cost: 0.1922405 | Acc train: 0.3530997 | Acc test: 0.1935484 \n",
      "Iter:   120 | time:3910.65779 |Cost: 0.0828724 | Acc train: 0.3517520 | Acc test: 0.1895161 \n",
      "Iter:   121 | time:3922.26846 |Cost: 0.1593235 | Acc train: 0.3598383 | Acc test: 0.2016129 \n",
      "Iter:   122 | time:3911.41067 |Cost: 0.1504867 | Acc train: 0.3719677 | Acc test: 0.2258065 \n",
      "Iter:   123 | time:3916.77613 |Cost: 0.1412888 | Acc train: 0.3692722 | Acc test: 0.2177419 \n",
      "Iter:   124 | time:3917.91978 |Cost: 0.1204231 | Acc train: 0.3719677 | Acc test: 0.2177419 \n",
      "Iter:   125 | time:3917.82476 |Cost: 0.1991554 | Acc train: 0.3652291 | Acc test: 0.2258065 \n",
      "Iter:   126 | time:3914.49538 |Cost: 0.1892053 | Acc train: 0.3571429 | Acc test: 0.2137097 \n",
      "Iter:   127 | time:3914.95130 |Cost: 0.3287795 | Acc train: 0.3638814 | Acc test: 0.2217742 \n",
      "Iter:   128 | time:3914.10468 |Cost: 0.2426197 | Acc train: 0.3530997 | Acc test: 0.2217742 \n",
      "Iter:   129 | time:3919.52555 |Cost: 0.1895880 | Acc train: 0.3584906 | Acc test: 0.2217742 \n",
      "Iter:   130 | time:3915.87180 |Cost: 0.2031757 | Acc train: 0.3544474 | Acc test: 0.2177419 \n",
      "Iter:   131 | time:3927.29290 |Cost: 0.1189436 | Acc train: 0.3719677 | Acc test: 0.2217742 \n",
      "Iter:   132 | time:3926.48744 |Cost: 0.3620146 | Acc train: 0.3450135 | Acc test: 0.2096774 \n",
      "Iter:   133 | time:3929.62041 |Cost: 0.2265283 | Acc train: 0.3544474 | Acc test: 0.2016129 \n",
      "Iter:   134 | time:3923.55540 |Cost: 0.2141821 | Acc train: 0.3611860 | Acc test: 0.1895161 \n",
      "Iter:   135 | time:3913.92311 |Cost: 0.0804018 | Acc train: 0.3638814 | Acc test: 0.1975806 \n",
      "Iter:   136 | time:3915.84490 |Cost: 0.1604910 | Acc train: 0.3733154 | Acc test: 0.1975806 \n",
      "Iter:   137 | time:3927.13391 |Cost: 0.1344800 | Acc train: 0.3746631 | Acc test: 0.1693548 \n",
      "Iter:   138 | time:3923.63126 |Cost: 0.1914754 | Acc train: 0.3638814 | Acc test: 0.1733871 \n",
      "Iter:   139 | time:3905.99237 |Cost: 0.1594879 | Acc train: 0.3557951 | Acc test: 0.1774194 \n",
      "Iter:   140 | time:3914.11964 |Cost: 0.1894772 | Acc train: 0.3571429 | Acc test: 0.1854839 \n",
      "Iter:   141 | time:3919.51058 |Cost: 0.1336944 | Acc train: 0.3652291 | Acc test: 0.1895161 \n",
      "Iter:   142 | time:3911.95693 |Cost: 0.1093596 | Acc train: 0.3652291 | Acc test: 0.1975806 \n",
      "Iter:   143 | time:3915.18198 |Cost: 0.1732971 | Acc train: 0.3584906 | Acc test: 0.2016129 \n",
      "Iter:   144 | time:3907.90922 |Cost: 0.2143085 | Acc train: 0.3625337 | Acc test: 0.1935484 \n",
      "Iter:   145 | time:3912.38348 |Cost: 0.1800906 | Acc train: 0.3692722 | Acc test: 0.1975806 \n",
      "Iter:   146 | time:3916.10237 |Cost: 0.1170322 | Acc train: 0.3719677 | Acc test: 0.1814516 \n",
      "Iter:   147 | time:3921.34906 |Cost: 0.2510430 | Acc train: 0.3611860 | Acc test: 0.1854839 \n",
      "Iter:   148 | time:3923.03399 |Cost: 0.1334795 | Acc train: 0.3760108 | Acc test: 0.1693548 \n",
      "Iter:   149 | time:3903.88372 |Cost: 0.1706271 | Acc train: 0.3814016 | Acc test: 0.1814516 \n",
      "Iter:   150 | time:3928.52870 |Cost: 0.2238294 | Acc train: 0.3840970 | Acc test: 0.1854839 \n",
      "Iter:   151 | time:3915.53359 |Cost: 0.1316328 | Acc train: 0.3962264 | Acc test: 0.1693548 \n",
      "Iter:   152 | time:3921.32623 |Cost: 0.1316784 | Acc train: 0.4029650 | Acc test: 0.1814516 \n",
      "Iter:   153 | time:3944.25180 |Cost: 0.1386981 | Acc train: 0.4110512 | Acc test: 0.1774194 \n",
      "Iter:   154 | time:3948.63929 |Cost: 0.2350812 | Acc train: 0.4056604 | Acc test: 0.1814516 \n",
      "Iter:   155 | time:3916.97543 |Cost: 0.1167778 | Acc train: 0.3948787 | Acc test: 0.1854839 \n",
      "Iter:   156 | time:3911.81468 |Cost: 0.2326792 | Acc train: 0.3921833 | Acc test: 0.1774194 \n",
      "Iter:   157 | time:3921.44155 |Cost: 0.1596580 | Acc train: 0.3989218 | Acc test: 0.1854839 \n",
      "Iter:   158 | time:3921.13014 |Cost: 0.1018552 | Acc train: 0.4056604 | Acc test: 0.1733871 \n",
      "Iter:   159 | time:3918.19032 |Cost: 0.0788620 | Acc train: 0.4056604 | Acc test: 0.1653226 \n",
      "Iter:   160 | time:3913.90139 |Cost: 0.1246105 | Acc train: 0.4002695 | Acc test: 0.1774194 \n",
      "Iter:   161 | time:3915.83828 |Cost: 0.1473939 | Acc train: 0.3894879 | Acc test: 0.1733871 \n",
      "Iter:   162 | time:3888.66542 |Cost: 0.1470062 | Acc train: 0.3814016 | Acc test: 0.1653226 \n",
      "Iter:   163 | time:3921.50840 |Cost: 0.1812077 | Acc train: 0.4164420 | Acc test: 0.1814516 \n",
      "Iter:   164 | time:3907.85823 |Cost: 0.0937807 | Acc train: 0.4110512 | Acc test: 0.2137097 \n",
      "Iter:   165 | time:3914.52438 |Cost: 0.1381343 | Acc train: 0.4123989 | Acc test: 0.2338710 \n",
      "Iter:   166 | time:3918.36937 |Cost: 0.2131645 | Acc train: 0.4097035 | Acc test: 0.2419355 \n",
      "Iter:   167 | time:3922.12157 |Cost: 0.0515134 | Acc train: 0.4083558 | Acc test: 0.2701613 \n",
      "Iter:   168 | time:3915.89158 |Cost: 0.1527926 | Acc train: 0.4123989 | Acc test: 0.2540323 \n",
      "Iter:   169 | time:3919.74006 |Cost: 0.1347870 | Acc train: 0.4177898 | Acc test: 0.2177419 \n",
      "Iter:   170 | time:3914.02461 |Cost: 0.1243751 | Acc train: 0.4043127 | Acc test: 0.2137097 \n",
      "Iter:   171 | time:3923.15518 |Cost: 0.1387011 | Acc train: 0.3962264 | Acc test: 0.1975806 \n",
      "Iter:   172 | time:3911.90718 |Cost: 0.1440475 | Acc train: 0.3935310 | Acc test: 0.1895161 \n",
      "Iter:   173 | time:3914.65509 |Cost: 0.2054727 | Acc train: 0.4056604 | Acc test: 0.1975806 \n",
      "Iter:   174 | time:3923.28349 |Cost: 0.0835526 | Acc train: 0.4164420 | Acc test: 0.2137097 \n",
      "Iter:   175 | time:3924.13693 |Cost: 0.1455328 | Acc train: 0.4070081 | Acc test: 0.2177419 \n",
      "Iter:   176 | time:3921.03360 |Cost: 0.1899026 | Acc train: 0.4056604 | Acc test: 0.2258065 \n",
      "Iter:   177 | time:3913.49719 |Cost: 0.1396717 | Acc train: 0.3989218 | Acc test: 0.2137097 \n",
      "Iter:   178 | time:3913.28036 |Cost: 0.1862327 | Acc train: 0.4043127 | Acc test: 0.1935484 \n",
      "Iter:   179 | time:3917.10089 |Cost: 0.1959460 | Acc train: 0.4070081 | Acc test: 0.1854839 \n",
      "Iter:   180 | time:3915.22373 |Cost: 0.3020234 | Acc train: 0.4150943 | Acc test: 0.1975806 \n",
      "Iter:   181 | time:3912.23225 |Cost: 0.1848632 | Acc train: 0.3975741 | Acc test: 0.1854839 \n",
      "Iter:   182 | time:3912.31129 |Cost: 0.0760431 | Acc train: 0.4002695 | Acc test: 0.2137097 \n",
      "Iter:   183 | time:3921.05275 |Cost: 0.1097039 | Acc train: 0.4164420 | Acc test: 0.2137097 \n",
      "Iter:   184 | time:3914.48235 |Cost: 0.2233023 | Acc train: 0.4285714 | Acc test: 0.2217742 \n",
      "Iter:   185 | time:3923.69295 |Cost: 0.1194745 | Acc train: 0.4353100 | Acc test: 0.2217742 \n",
      "Iter:   186 | time:3908.57525 |Cost: 0.1321906 | Acc train: 0.4299191 | Acc test: 0.2338710 \n",
      "Iter:   187 | time:3926.90120 |Cost: 0.0684811 | Acc train: 0.4487871 | Acc test: 0.2177419 \n",
      "Iter:   188 | time:3926.96429 |Cost: 0.0921099 | Acc train: 0.4474394 | Acc test: 0.2096774 \n",
      "Iter:   189 | time:3911.83986 |Cost: 0.0938859 | Acc train: 0.4407008 | Acc test: 0.2016129 \n",
      "Iter:   190 | time:3902.90320 |Cost: 0.0642596 | Acc train: 0.4420485 | Acc test: 0.1895161 \n",
      "Iter:   191 | time:3929.05661 |Cost: 0.1436944 | Acc train: 0.4326146 | Acc test: 0.1975806 \n",
      "Iter:   192 | time:3917.02993 |Cost: 0.1279988 | Acc train: 0.4231806 | Acc test: 0.1935484 \n",
      "Iter:   193 | time:3915.49732 |Cost: 0.1945038 | Acc train: 0.4218329 | Acc test: 0.2016129 \n",
      "Iter:   194 | time:3925.08310 |Cost: 0.1879327 | Acc train: 0.3948787 | Acc test: 0.2056452 \n",
      "Iter:   195 | time:3907.08507 |Cost: 0.1307642 | Acc train: 0.3948787 | Acc test: 0.1935484 \n",
      "Iter:   196 | time:3951.94995 |Cost: 0.1175561 | Acc train: 0.3908356 | Acc test: 0.1935484 \n"
     ]
    }
   ],
   "source": [
    "#train modolFalse\n",
    "rerun = False\n",
    "start3 = time.time()\n",
    "training(features, Y,rerun)\n",
    "end3 = time.time()\n",
    "\n",
    "print('total run time:',(end3 -start3 )/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
